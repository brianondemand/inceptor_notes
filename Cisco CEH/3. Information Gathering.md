
## Reconnaissance

Reconnaissance is always the initial step in a cyber attack. 

Reconnaissance involves gathering information about a target before launching an attack. 

Reconnaissance in a penetration testing engagement typically consists of scanning and enumeration.


**_Active reconnaissance_** is a method of information gathering in which the tools used actually send out probes to the target network or systems in order to elicit responses that are then used to determine the posture of the network or system. 

Common active reconnaissance tools and methods include the following:

- Host enumeration
- Network enumeration
- User enumeration
- Group enumeration
- Network share enumeration
- Web page enumeration
- Application enumeration
- Service enumeration
- Packet crafting

**_Passive reconnaissance_** is a method of information gathering in which the tools do not interact directly with the target device or network. There are multiple methods of passive reconnaissance. Some involve using third-party databases to gather information. 

Common passive reconnaissance tools and methods include the following:

- Domain enumeration
- Packet inspection
- Open-source intelligence (OSINT)
- Recon-ng
- Eavesdropping

---

## Performing Passive Reconnaissance


### 1. Using OSINT Tools

OSINT stands for **Open Source Intelligence**, and it's the practice of gathering information from publicly available sources to gain actionable insights. 

This includes websites, social media, public records, news outlets, and more. The goal is to collect valuable intelligence without breaching any laws or engaging in unauthorized activities.

OSINT is widely used in penetration testing as part of the **passive reconnaissance phase** to understand the target without direct engagement.

---
#### The OSINT Framework

The OSINT Framework ([https://osintframework.com/](https://osintframework.com/)) is an interactive, tree-structured map of OSINT resources. Although some links may be outdated, it remains a valuable tool for understanding the categories and use cases of OSINT tools.

- It categorizes tools by type, such as **Username**, **Email Addresses**, **IP Addresses**, and more.

- Clicking on a node (e.g., “Username”) reveals subcategories and tools relevant to that area.

- Example Tool: **WhatsMyName**


#### Using WhatsMyName for Username Enumeration

WhatsMyName is a username search tool that checks the availability of a username across hundreds of online platforms. This helps in identifying which services an individual or organization may be using.

- The tool is accessible at: [https://whatsmyname.app/](https://whatsmyname.app/)

- It generates a report showing where a given username exists, with links to each account.

- It also allows exporting the results and filtering by platform type.


**Value of Username Searches:**  
Username enumeration can uncover personal and corporate accounts tied to a target, enabling social engineering, password reuse testing, and identifying employee activities across social platforms.


---

### Using SpiderFoot

SpiderFoot is a powerful automated OSINT reconnaissance tool built into Kali Linux. It can run both through a GUI and the terminal, and it queries over 1000 public sources to uncover relationships between data points.

SpiderFoot can investigate:

- Domains

- IP addresses

- Subnets

- ASN (Autonomous System Numbers)

- Email addresses

- Personal names

- Phone numbers


**SpiderFoot Use Cases**

SpiderFoot allows scans based on the following **use cases**:

- **All**: Performs a comprehensive scan using all modules (can take hours).

- **Footprint**: Focuses on the network perimeter and associated entities.

- **Investigate**: Targets potentially malicious entities using blacklists and security feeds.

- **Passive**: Gathers data in a stealthy way, minimizing detection risk.


**Running SpiderFoot**

To start SpiderFoot, open the terminal and run:

```
spiderfoot -l 127.0.0.1:5001
```

This launches the local GUI on port 5001. Access it via browser at: `http://127.0.0.1:5001`

Once running, you can:

- Create new scans by target (e.g., domain `h4cker.org`)

- Choose the scan type (e.g., Footprint or Passive)

- Monitor real-time scan progress via graphs and data summaries



**Exploring the SpiderFoot Modules**

SpiderFoot uses over 200 modules (e.g., DNS checks, social media, leaked credentials). Each module has a name and a corresponding code reference in the format `sfp_[modulename]`.

Modules can be:

- **Free and built-in**

- **API-based**, requiring registration and keys (e.g., Shodan, HaveIBeenPwned, Google Search API)


You can list modules via:

```bash
spiderfoot -M
```

And search them using:

```bash
spiderfoot -M | grep email
```

Each module includes documentation on what data it pulls, what it requires, and what triggers it.



**SpiderFoot Scan Example: h4cker.org**

1. Create a new scan

2. Choose “Footprint” use case

3. Run the scan

4. Let it run for 30+ minutes for best results


During the scan:

- SpiderFoot actively collects DNS, WHOIS, IP, netblocks, social profiles, leaks, and more.

- Data is categorized and browsable in tables and graphs.

- Scan data includes module source, type of data (e.g., IPs, emails), and results.


---

**API Key Integration**

Some modules require **API keys** to unlock advanced functionality. These can typically be acquired via free registration on third-party websites.

Examples of modules needing API keys:

- `sfp_shodan` (requires a Shodan API key)

- `sfp_haveibeenpwned` (uses HaveIBeenPwned data)

- `sfp_virustotal` (VirusTotal file and domain intelligence)

- `sfp_clearbit` (company info and brand intelligence)


You can input keys under the **Settings > API Key** section in the GUI, then selectively run scans using only those modules.

---

**Analyzing Results**

Once a scan is complete:

- Browse results under the **Browse** tab

- View data by module, data type, and source

- Search for specific keywords or data types (e.g., leaks, DNS records)

- For any “Leak Site URL” results, check the **Source Module** to determine which module found the data (e.g., `sfp_haveibeenpwned`)


This allows you to trace what data was found, where it came from, and why it might be a security concern.

---

## 2. DNS Lookups

**DNS (Domain Name System)** is like the phonebook of the internet. Instead of remembering complex IP addresses like `185.199.108.153`, people use domain names like `www.google.com`. DNS translates these human-friendly domain names into machine-readable IP addresses that computers use to communicate with each other.

For example:

- You type `www.google.com` into your browser.

- DNS finds the matching IP address, like `142.250.190.68`.

- Your browser connects to that IP to load the website.

A **DNS Lookup** is the process of querying the DNS system to resolve (or find) the **IP address** of a domain name. It helps identify where the domain is hosted and what services it runs.

There are two main types:

1. **Forward DNS Lookup** – Given a domain (e.g., `example.com`), it returns the IP address.

2. **Reverse DNS Lookup** – Given an IP address, it returns the associated domain name (if any).

You can use other basic DNS tools, such as the **nslookup**, **host**, and **dig** Linux commands, to perform name resolution and obtain additional information about a domain.

You can easily identify domain technical and administrative contacts by using the **Whois tool**. Many organizations keep their registration details private and instead use the domain registrar organization contacts.

Various tools, such as Recon-ng, **the Harvester**, and **Maltego**, help automate the process of passive reconnaissance and support many DNS-based and Whois queries.

---

### Practical Session


#### 1. Using nslookup to Obtain Domain and IP Address Information

`nslookup` is a command-line tool used to query DNS records for a domain name.

**Starting `nslookup`**

- Open the terminal and type `nslookup` to start the interactive mode.

```bash
$ nslookup
```


**Querying Domain Information**

- To query the IP address of a domain, simply enter the domain name after starting `nslookup`. This returns the A (IPv4) and AAAA (IPv6) records.

    ```bash
    > cisco.com
    ```

- The output will provide the IP addresses associated with the domain:

    ```
    Server:         192.168.1.1
    Address:        192.168.1.1#53
    
    Non-authoritative answer:
    Name:   cisco.com
    Address: 72.163.4.185  (IPv4 Address)
    Name:   cisco.com
    Address: 2001:420:1101:1::185  (IPv6 Address)
    ```

**Querying for Name Servers (NS Records)**

- To query for the domain's Name Servers, use the `set type=ns` command:

    ```bash
    > set type=ns
    > cisco.com
    ```

    Output example:

    ```
    cisco.com nameserver = ns1.cisco.com.
    cisco.com nameserver = ns2.cisco.com.
    ```


**Switching to a Different DNS Server**

- You can query using a different DNS server. To use Google's DNS server (8.8.8.8) to resolve a domain:

    ```bash
    $ nslookup skillsforall.com 8.8.8.8
    ```


**Querying All DNS Records**

- To get all records for a domain, use `set type=any`:

    ```bash
    > set type=any
    > skillsforall.com
    ```


**Exiting Interactive Mode**

- To exit the interactive mode, simply type `exit`:

    ```bash
    > exit
    ```



#### 2. Using Whois to Obtain Domain Registration Information

`whois` is a command-line tool that allows you to query domain registration details, such as the domain owner, registration dates, and contact information.

**Running Whois Command**

- To query domain registration information for `cisco.com`, use:

    ```bash
    $ whois cisco.com
    ```

    Example output:

    ```
    Domain Name: CISCO.COM
    Registrar: MARKMONITOR INC.
    Creation Date: 1987-01-25
    Updated Date: 2021-10-12
    ```


**Comparing Whois Output for Multiple Domains**

- You can run the `whois` command for another domain like `skillsforall.com`:

    ```bash
    $ whois skillsforall.com
    ```

- This will show different registration details such as the registrar and creation date. You can use this to compare the two domains.


**Finding IP Address Range Information**

- `whois` can also be used to find IP address ranges. After resolving the DNS server IP (like `72.163.5.201` for `ns1.cisco.com`), you can get the IP range assigned to Cisco:

    ```bash
    $ whois 72.163.5.201
    ```

    Example output:

    ```
    NetRange:       72.163.0.0 - 72.163.255.255
    CIDR:           72.163.0.0/16
    ```



#### 3. Using Dig for DNS Queries

`dig` (Domain Information Groper) is another DNS query tool available on Linux. It provides more detailed information than `nslookup`.

**Querying DNS Records with Dig**

- To resolve the domain `cisco.com` to an IP address using `dig`, type:

    ```bash
    $ dig cisco.com
    ```

    Example output:

    ```
    ;; ANSWER SECTION:
    cisco.com.             86400   IN      A       72.163.4.185
    ```


**Querying Specific DNS Record Types**

- You can specify the type of record you want to query. For example, to get the IPv6 address (AAAA record):

    ```bash
    $ dig cisco.com AAAA
    ```


**Using a Specific DNS Server with Dig**

- You can query a specific DNS server (e.g., Google DNS 8.8.8.8) using the following syntax:

    ```bash
    $ dig cisco.com @8.8.8.8
    ```

- To query the DNS server records (NS records) for `cisco.com` using the Google DNS server, use:

    ```bash
    $ dig cisco.com @8.8.8.8 ns
    ```


**Output of Dig Command**

- `dig` gives more detailed information, such as the TTL (time-to-live), record type, and authoritative answers:

    ```
    ;; ANSWER SECTION:
    cisco.com.             86400   IN      NS      ns1.cisco.com.
    ```


**Differences Between `nslookup` and `dig`**

- `nslookup` is more user-friendly, but `dig` provides more detailed information.

- By default, `dig` queries more record types and gives additional information, such as query time, flags, and additional sections, compared to `nslookup`.


#### 4. Using WHOIS for Domain Registration Information

The `whois` command is a tool used to retrieve domain registration information, including details about the domain owner, registrar, creation and expiration dates, and more. It can also provide information about IP address ranges and network ownership.

- **Basic Query**:

    ```bash
    whois example.com
    ```

- **Query a Specific WHOIS Server**:

    ```bash
    whois -h whois.nic.fr example.fr
    ```

- **Query IP Address**:

    ```bash
    whois 72.163.4.185
    ```

- **Verbose Output**:

    ```bash
    whois -v example.com
    ```

---


## 3. Social Media Scraping

Social media scraping involves attackers collecting publicly available information from platforms like Twitter, LinkedIn, Facebook, and Instagram to gather personal and professional data about individuals and organizations. This information can then be exploited for malicious purposes, such as social engineering attacks.

#### **Key Points:**

1. **Types of Information Scraped:**

    - **Personal Information**: People often share details like hobbies, work promotions, travel plans, and dining preferences, which attackers can use to tailor attacks.
    
    - **Professional Information**: Attackers gather data about job roles, responsibilities, and key contacts, which are crucial for targeted social engineering.

2. **Use of Information for Social Engineering Attacks:**

    - **Spear Phishing and Whaling**: Attackers use personal and professional information to craft convincing, targeted attacks, such as spear phishing (targeted emails) and whaling (targeting high-level executives).

3. **Job Listings and Technology Stacks:**

    - **Scraping Job Listings**: Attackers monitor job boards (like LinkedIn, Indeed, CareerBuilder) to identify the technologies and systems used by companies. For example, a job listing for a Cisco firewall administrator indicates that the company uses Cisco firewalls, providing attackers with valuable insight into the company’s infrastructure.

4. **Fake Job Postings:**

    - **Trapping Victims**: Attackers sometimes create fake job listings to lure potential employees. During interviews, attackers try to extract information about the technologies, applications, and systems the candidate’s company uses. Job seekers, eager to impress, may inadvertently disclose sensitive details.

---

## 4. Cryptographic Flaws

During the reconnaissance phase, attackers often can inspect Secure Sockets Layer (SSL) certificates to obtain information about the organization, potential cryptographic flaws, and weak implementations. 

You can find a lot inside digital certificates: the certificate serial number, the subject common name, the uniform resource identifier (URI) of the server it was assigned to, the organization name, Online Certificate Status Protocol (OCSP) information, the certificate revocation list (CRL) URI, and so on.

**Certificate revocation** is the act of invalidating a digital certificate. For instance, if an application has been decommissioned or the certificate assigned to such application is compromised, you should revoke the certificate and add its serial number to a CRL. OCSP and CRLs are used to verify whether a certificate has been revoked (that is, invalidated) by the issuing authority.

A **digital certificate** is like an online ID card for websites or apps. It helps to prove that they are who they say they are.

- **CRL (Certificate Revocation List)**: This is a list maintained by the certificate authority (CA) that shows which digital certificates are no longer valid or have been revoked. It’s like a blacklist for certificates that are no longer trusted.
    
- **OCSP (Online Certificate Status Protocol)**: This is a protocol used to check if a certificate is still valid in real-time. Instead of checking a list like CRL, it asks the certificate authority directly if a certificate is still valid.
    
- **CA (Certificate Authority)**: This is the organization or entity that issues digital certificates. They are responsible for validating the identity of the certificate holder and managing the revocation process.

**Certificate Transparency** was created to improve the security of digital certificates after attacks like the one against DigiNotar. Its goal is to allow anyone to easily verify the certificates issued for a domain, making it harder for malicious certificates to go unnoticed. While this helps detect fraudulent certificates, attackers can also use it to gather information about other subdomains or systems owned by an organization.


**Using Certificate Transparency (CT) to Find Subdomains**

Tools such as **crt.sh** enable you to obtain detailed certificate transparency information about any given domain.

Visit `https://crt.sh` to search for SSL certificates associated with a domain and explore subdomains.

#### Finding Information from SSL Certificates

**SSL certificates** are an essential part of online security. They are used to both encrypt data as it transmitted and establish that a website can be trusted as a source or destination for data. It is important to understand the identification and encryption information that is available in SSL certificates.

**SSL/TLS Certificates**: These provide validation for websites, ensuring secure communication between clients and servers. Ethical hackers use SSL information during penetration tests to uncover details about a website, such as domain names, expiration dates, and potential vulnerabilities.

**Viewing Certificate Information:**

- **From Browsers**: Certificates can be viewed by clicking the padlock icon in the URL bar. This shows certificate details like domain, issuer, expiration date, and encryption algorithm.

- **On Local Hosts**: On Windows, certificates are managed using `certmgr.msc`. In Kali Linux, certificates are stored in `/usr/share/ca-certificates/mozilla`.


**SSL Analysis Tools in Kali Linux**:

- **Tools like sslscan**: Kali Linux provides several SSL tools to gather detailed certificate information, including `sslscan`, a command-line utility.

- **Using aha for Output Formatting**: `aha` is used to convert terminal output into an HTML file, preserving color coding for easier interpretation.

```bash
sudo apt install -y aha
```

```bash
sslscan skillsforall.com | aha > sfa_cert.html
```

**Important Tools:**

- **sslscan**: A tool for scanning SSL certificates.

- **aha**: A utility to output terminal results as HTML.


---


## 5. Company Reputation and Security Posture

Security breaches can significantly damage a company's reputation. Attackers often use data from previous breaches to gather information, including password dumps, file metadata, and website archives.

They may, for example, leverage the following data while trying to gather information about their victims:

- Password dumps
- File metadata
- Strategic search engine analysis/enumeration
- Website archiving/caching
- Public source code repositories


#### Password dumps

Password dumps from past breaches can be accessed through various tools and websites, such as Pastebin, the dark web, or even GitHub. 


The following are additional tools that allow you to search for breach data dumps:

- **WhatBreach:** _[https://github.com/Ekultek/WhatBreach](https://github.com/Ekultek/WhatBreach)_
- **LeakLooker:** _[https://github.com/woj-ciech/LeakLooker](https://github.com/woj-ciech/LeakLooker)_
- **Buster:** _[https://github.com/sham00n/buster](https://github.com/sham00n/buster)_
- **Scavenger:** _[https://github.com/rndinfosecguy/Scavenger](https://github.com/rndinfosecguy/Scavenger)_
- **PwnDB:** _[https://github.com/davidtavarez/pwndb](https://github.com/davidtavarez/pwndb)_



> A tool like **h8mail** allows attackers to find exposed email addresses and passwords from these breaches. Other tools like WhatBreach and LeakLooker also assist in finding breach data.

#### h8mail

This tool allows you to find email addresses and passwords exposed in previous breaches.

**Install h8mail:**

```bash
pip3 install h8mail
```

**Use h8mail to find breaches:**

```bash
h8mail -t user@example.com
```

You can also search for multiple email addresses, URLs, or perform custom queries:

```bash
h8mail -t "user1@example.com user2@example.com" -u "http://example.com"
```

- `-h` : Show help

- `-o OUTPUT_FILE` : Output results to a file in CSV format

- `-j OUTPUT_JSON` : Output results in JSON format

- `-c CONFIG_FILE` : Use API configuration files for breach sources (e.g., Snusbase, WeLeakInfo)

##### WhatBreach

This tool helps search for breach data dumps.

**Clone the repository:**

```bash
git clone https://github.com/Ekultek/WhatBreach.git
cd WhatBreach
```

**Use the tool:**

```bash
python3 WhatBreach.py -t user@example.com
```


##### LeakLooker

LeakLooker is another tool for searching for breached data.

**Clone the repository:**

```bash
git clone https://github.com/woj-ciech/LeakLooker.git
cd LeakLooker
```

**Run the tool:**

```bash
python3 leaklooker.py -t user@example.com
```

##### Buster

This is a tool for searching for leaked data.

**Clone the repository:**

```bash
git clone https://github.com/sham00n/buster.git
cd buster
```

**Use the tool:**

```bash
python3 buster.py -t user@example.com
```


##### Scavenger

Scavenger is used to search for leaked data and related information.

**Clone the repository:**

```bash
git clone https://github.com/rndinfosecguy/Scavenger.git
cd Scavenger
```

**Run the tool:**

```bash
python3 scavenger.py -t user@example.com
```


##### PwnDB

PwnDB allows you to search a breached database for user information.

**Clone the repository:**

```bash
git clone https://github.com/davidtavarez/pwndb.git
cd pwndb
```

**Use the tool:**

```bash
python3 pwnDB.py -t user@example.com
```


These commands allow you to search for breached data using the mentioned tools. Ensure you have the required dependencies installed and are following legal and ethical guidelines when conducting searches.


#### File Metadata

File metadata, particularly in image files, can reveal sensitive information. Exif (Exchangeable Image File Format) metadata in images, for example, can provide details like camera model, software used, GPS coordinates, and other technical information. 

> Tools like **ExifTool** can extract this metadata from files to obtain such details, potentially revealing personal or corporate data unintentionally exposed through images.

These data points, if not properly protected, can be exploited by attackers to launch further attacks, compromise systems, and harm the organization's reputation.


#### Strategic Search Engine Analysis/Enumeration

Google Hacking, or Google Dorking, is a technique used by security researchers and attackers to find vulnerabilities or sensitive information by leveraging advanced Google search operators. These operators allow you to search more specifically within files, URLs, or even parts of a webpage's content. The most common Google operators are:

**1. `filetype:`**

The `filetype:` operator allows you to restrict your search to a specific file type. It is useful when you are trying to locate certain types of documents.

- Example:
    ```plaintext
    filetype:xls "company financials"
    ```

- This searches for Excel spreadsheets containing the phrase "company financials."


**2. `inurl:`**

The `inurl:` operator limits search results to pages where a specific word or phrase appears in the URL.

- Example:

    ```plaintext
    inurl:admin
    ```

- This search looks for URLs containing the word "admin," which might point to admin pages.


**3. `link:`**

The `link:` operator is used to find links to a specific URL.

- Example:

    ```plaintext
    link:www.example.com
    ```

- This will show pages that contain links to `www.example.com`.


**4. `intitle:`**

The `intitle:` operator restricts search results to pages where a specified word or phrase appears in the title of the page.

- Example:

    ```plaintext
    intitle:"Index of /etc"
    ```

- This search will look for pages where the title is exactly "Index of /etc," which could point to directory listings or configuration files.


**5. `intext:`**

The `intext:` operator restricts search results to pages that contain specific text in the body.

- Example:

    ```plaintext
    intext:"password"
    ```

- This searches for pages that contain the word "password" somewhere in the body, potentially revealing password files.


**6. `OR` Operator**

You can combine multiple search terms with the `OR` operator, which allows Google to find pages that contain either one of the search terms.

- Example:

    ```plaintext
    intext:"user" OR intext:"admin"
    ```

- This will find pages that contain either "user" or "admin."



**Google Dorks Examples**

**1. Session ID Leaks:**

```plaintext
intext:JSESSIONID OR intext:PHPSESSID inurl:access.log ext:log
```

This search looks for session IDs in log files that are publicly accessible. Session IDs can be used by attackers to impersonate users if they haven't expired.

**2. Exposed Database Credentials:**

```plaintext
"public $user =" | "public $password = " | "public $secret =" | "public $db =" ext:txt | ext:log -git
```

This search looks for plain-text credentials (usernames, passwords, secrets) in `.txt` or `.log` files, which might be accidentally exposed on the internet.

**3. Password Files or Databases:**

```plaintext
"public $password =" ext:sql | ext:php
```

This search looks for password variables within SQL or PHP files, which may indicate exposed sensitive data.

---

#### The Google Hacking Database (GHDB)

The **Google Hacking Database (GHDB)** is a collection of Google Dorks that were compiled by security experts and shared publicly. The GHDB contains different categories of dorks, such as:

- **Footholds:** Pages that can give attackers initial access to systems.

- **Sensitive Directories:** Directories with sensitive files.

- **Web Server Detection:** Find details about web server software.

- **Error Messages:** Error messages that may reveal information about server configurations.

- **Files Containing Passwords:** Plain-text password files exposed in search results.

- **Pages Containing Login Portals:** Pages with login forms that may be vulnerable to brute force or other attacks.


You can access the GHDB at [Exploit-DB Google Hacking Database](https://www.exploit-db.com/google-hacking-database/).

**Additional Tools for Google Hacking:**

In addition to using Google itself for dorking, there are tools that can help automate or extend this process:

1. **Recon-ng:** A web reconnaissance tool that automates many of the tasks of Google hacking and information gathering. It has modules for Google dorking and other reconnaissance techniques.

2. **TheHarvester:** This tool can be used for email gathering, DNS enumeration, and other reconnaissance tasks, including Google dorking.

---

#### The Wayback Machine

The **Wayback Machine** by the **Internet Archive** is an invaluable tool for viewing and accessing past versions of websites. It allows users to "go back in time" and see how websites looked at different points in history. By storing snapshots of websites, the Wayback Machine preserves the web's history and provides access to data that might otherwise be lost.

**Here’s how it works:**

- The Wayback Machine crawls and archives web pages by capturing and storing copies of them at regular intervals.

- Users can search for a website and view its history, from its earliest archived versions to more recent snapshots.

- You can see what a website looked like at any given point in time, including its design, content, and structure.


**Uses of the Wayback Machine:**

1. **Website History & Research:**

- Users can research how websites have evolved over time.

- Can be useful for analyzing older website designs, features, and content.

2. **Accessing Lost Information:**

- Sometimes websites go offline or lose important content. The Wayback Machine can help retrieve information that was once available but is no longer accessible.

3. **Digital Preservation:**

- Archiving web pages ensures that content is preserved for future generations, especially for sites that may no longer be maintained.

4. **SEO & Digital Marketing:**

- Marketers can analyze the historical SEO strategies and changes in content over time on competitor websites.


**Limitations:**

- **Incomplete Archives:** Not every website is fully archived. Some sites may not have been crawled, and some content (like dynamic content or databases) might not be captured correctly.

- **Old Versions:** The Wayback Machine may not have every single version of a site. It depends on the frequency of archiving.


**Other Website Archiving Services:**

- **Google Cache:** Similar to the Wayback Machine, Google’s cache allows users to view a snapshot of a page as Google last indexed it.

- **Archive.today:** Another service that captures and stores web pages, offering its own method of archiving.

- **Memento Web:** Memento aggregates archives from various repositories, including the Wayback Machine.


---

#### Public Source Code Repositories

An attacker can obtain extremely valuable information from public source code repositories such as GitHub and GitLab. Most of the applications and products we consume today use open-source software that is freely available in these public repositories. 

Attackers can find vulnerabilities in those software packages and use them to their advantage. Similarly, as a penetration tester, you can obtain valuable information from these public repositories. 

Even if you do not immediately find security vulnerabilities in the code, these repositories can give you insights into the architecture and underlying code used in the organization’s applications and infrastructure.

---


## Open-Source Intelligence (OSINT) Gathering

**_Open-source intelligence (OSINT) gathering_** is a method of gathering publicly available intelligence sources to collect and analyze information about a target. OSINT is “open source” because collecting the information does not require any type of covert methods. 

Typically, the information can be found on the Internet. The larger the online presence of the target, the more information that will be available. This type of collection can often start with a simple Google search, which can reveal a significant amount of information about a target.

It will at least give you enough information to know what direction to go with your information-gathering process.

### Recon-ng

It is a framework developed by Tim Tomes of Black Hills Information Security. This tool was developed in Python with Metasploit **msfconsole** in mind. If you have used the Metasploit console before, Recon-ng should be familiar and easy to understand.

Recon-ng is a modular framework, which makes it easy to develop and integrate new functionality. 

Recon-ng is incredibly powerful because it uses the APIs of various OSINT resources to gather information. Its modules can query sites such as Facebook, Indeed, Flickr, Instagram, Shodan, LinkedIn, and YouTube.

It also includes a reporting feature that allows you to export data in different report formats.



**Start Recon-ng**

```
recon-ng 
```

**View available commands**

```
help
```

- To get an idea of what commands are available in the Recon-ng command-line tool.

**Search for available modules**

```
marketplace search
```

- Recon-ng comes with a “marketplace,” where you can search for available modules to be installed. 

**Refresh the marketplace**

```
marketplace refresh
```

- You can refresh the data about the available modules by using the marketplace refresh command.

**Search the marketplace**

```
marketplace search bing
```

- You can perform a keyword search for any modules by using the command marketplace search < keyword >.

**Install a module**

```
marketplace install recon/domains-hosts/bing_domain_web
```

- You can install the module by using the marketplace install command.

**Show installed modules**

```
modules search
```

- You can use the modules search command to show all the modules that have been installed in Recon-ng.

**Load a module**

```
modules load recon/domains-hosts/bing_domain_web
```

- To load the module that you would like to use, use the modules load command.

**Change the source**

```
options set SOURCE h4cker.org
```

- You can change the source (the domain to be used to find its subdomains) by using the command options set SOURCE.


---

You can also combine other modules to find additional information about a specific target. 

For example, you can use the recon/domains-hosts/brute_hosts module to use wordlists and perform DNS queries to find additional subdomains. 

Hacking is all about the process of thinking like an attacker and developing a good methodology. I strongly recommend that you "go beyond the tool" and default behavior and develop your own methodology by combining tools and other resources to find information and potential vulnerabilities to exploit.

---

## Shodan

Shodan is an organization that scans the Internet 24 hours a day, 365 days a year. The results of those scans are stored in a database that can be queried at shodan.io or by using an API. 

You can use Shodan to query for vulnerable hosts, Internet of Things (IoT) devices, and many other systems that should not be exposed or connected to the public Internet.

Different categories of systems found by Shodan scans, including industrial control systems (ICS), databases, network infrastructure devices, and video games.


![][https://www.netacad.com/content/eh/1.0/courses/content/m3/en-US/assets/9065f6b147b007268e132b83169a937efb5b9516.png]

---

Keep in mind that even though this is public information, you should not interact with any systems shown in Shodan results without permission from the owner.

Shodan is an amazing tool if used correctly. It can uncover serious vulnerabilities in services running on internet-connected servers from around the world. However, in order for it to yield useful results, you need to understand how it works and the search filters to use to get the information that you want.

---

## Performing Active Reconnaissance

The process of gathering this information is called **enumeration**.

External enumeration of hosts is usually one of the first things you do in a penetration test. Determining the Internet-facing hosts of a target network can help you identify the systems that are most exposed.


### Port Scans

A **_port scan_** is an active scan in which the scanning tool sends various types of probes to the target IP address and then examines the responses to determine whether the service is actually listening. For instance, with an Nmap SYN scan, the tool sends a TCP SYN packet to the TCP port it is probing. 

This process is also referred to as half-open scanning because it does not open a full TCP connection. If the response is a SYN/ACK, this would indicate that the port is actually in a listening state. 

If the response to the SYN packet is an RST (reset), this would indicate that the port is closed or is not in a listening state. If the SYN probe does not receive any response, Nmap marks it as filtered because it cannot determine if the port is open or closed.

|**Nmap Port Status Reported**|**Response from Target**|**Nmap Analysis**|
|---|---|---|
|Open|TCP SYN-ACK|The service is listening on the port.|
|Closed|TCP RST|The service is not listening on the port.|
|Filtered|No response from target or ICMP destination unreachable|The port is firewalled.|


**Nmap SYN Scan Illustration**

![][https://media.geeksforgeeks.org/wp-content/uploads/20220715123349/synscanning1.png]

---

## Nmap Scan Types


#### TCP Connect Scan

The TCP Connect Scan uses the operating system's TCP stack to complete a full three-way handshake with the target. This scan is automatically used when you run Nmap without root or administrative privileges. It is reliable and accurate, but easily detected and logged by target systems.

Command and option:  
`nmap -sT <target>`  

Example: `nmap -sT 192.168.1.1`

---

#### UDP Scan

The UDP Scan is used to identify open UDP ports. Nmap sends empty UDP packets to each port and monitors for ICMP "port unreachable" replies. If no response is received, the port is considered open|filtered. UDP scans are slower due to limited feedback and potential rate limiting by the target.

Command and option:  

`nmap -sU <target>`  

Example: `nmap -sU 192.168.1.1`

To scan both TCP and UDP ports:  

`nmap -sS -sU -p T:1-100,U:1-100 <target>`

---

#### TCP FIN Scan

The TCP FIN Scan attempts to evade basic firewalls and packet filters. Instead of initiating a connection with a SYN packet, it sends a FIN packet. According to TCP specifications, closed ports respond with RST, while open ports remain silent. This makes it stealthier, but it does not work reliably against Windows targets.

Command and option:  

`nmap -sF <target>`  

Example: `nmap -sF 192.168.1.1`

---

#### Host Discovery Scan

The Host Discovery Scan is used to determine which systems on a network are alive. It does not perform a port scan. By default, Nmap sends an ICMP echo request, TCP SYN to port 443, TCP ACK to port 80, and ICMP timestamp request. Responses to any of these indicate the host is up.

Command and option:  

- [ ] `nmap -sn <target>`  

Example: `nmap -sn 192.168.1.0/24`

To perform host discovery with a more aggressive scan:  

`nmap -Pn <target>`  

This skips host discovery and treats all hosts as online.

---

#### Port Scanning Behavior

By default, Nmap scans the top 1000 most commonly used TCP ports. This behavior offers a balance of speed and effectiveness for general network reconnaissance. To scan a specific port range or the entire port space, use the `-p` option.

Scan top 1000 ports (default):  

`nmap <target>`

Scan all 65535 TCP ports:  

`nmap -p- <target>`

Scan a custom port range:  

`nmap -p 1-1000 <target>`

Scan specific ports:  

`nmap -p 22,80,443 <target>`

---

#### Service and Version Detection

To detect services and versions running on open ports, Nmap uses banner grabbing and service probes.

Command and option:  

`nmap -sV <target>`  

Example: `nmap -sV 192.168.1.1`

---

#### Operating System Detection

This feature attempts to determine the operating system of the target host using TCP/IP stack fingerprinting.

Command and option:  

`nmap -O <target>`  

Example: `nmap -O 192.168.1.1`

For more accuracy, combine service and OS detection:  

`nmap -A <target>`

---

#### Aggressive Scan

An aggressive scan enables OS detection, version detection, script scanning, and traceroute in one go. It is very noisy and easily detected but gives detailed results.

Command and option:  

`nmap -A <target>`  

Example: `nmap -A 192.168.1.1`

---

#### Save Scan Results

Nmap allows you to save your output in different formats for further analysis.

Save as normal text:  

`nmap -oN scan.txt <target>`

Save as XML:  

`nmap -oX scan.xml <target>`

Save in grepable format:  

`nmap -oG scan.gnmap <target>`

Save all formats at once:  

`nmap -oA fullscan <target>`  

This will generate `fullscan.nmap`, `fullscan.xml`, and `fullscan.gnmap`

---

## Types of Enumeration

The process of gathering this information is called **enumeration**.

- Host Enumeration
- User Enumeration
- Group Enumeration
- Network Share Enumeration
- Additional SMB Enumeration Examples
- Web Page Enumeration/Web Application Enumeration
- Service Enumeration
- Exploring Enumeration via Packet Crafting

---

#### Host Enumeration

Host enumeration is a foundational step in the information-gathering phase of a penetration test, used to identify live hosts within a target network. This process can be performed both externally and internally. 

External enumeration should be limited strictly to in-scope IP addresses to avoid scanning unauthorized systems. Internally, however, it typically involves scanning entire subnets used by the target. Tools commonly used for host enumeration include **Nmap** and **Masscan**. For instance, an Nmap ping scan of a network like `192.168.88.0/24` can be executed with the command:  

```
nmap -sn 192.168.88.0/24
```

The `-sn` option (formerly `-sP`) tells Nmap to skip port scanning and only perform host discovery (ping sweep). This helps identify which hosts are active without probing for open ports. Alternatively, **Masscan** can be used for high-speed scanning:  

```
masscan 192.168.88.0/24 -p0-65535
```

This scans all ports and hosts within the subnet very quickly, though it can be noisy and may require rate-limiting options like `--rate`.

---

#### User Enumeration

User enumeration is crucial once internal access is obtained, as discovering valid usernames enables targeted brute-force attacks. On Windows networks, user enumeration can be done by exploiting the **Server Message Block (SMB)** protocol, which operates over **TCP port 445**. Enumeration through SMB involves sending crafted requests and analyzing the server's responses.

The **SMB_COM_NEGOTIATE** message initiates the negotiation process, where the client and server agree on protocol options. Misconfigured servers might reveal whether message signing is enforced, what authentication levels (user- or share-level) are supported, and whether plaintext passwords are accepted. The server might also disclose its system time and timezone, which can assist in other attacks.

After negotiation, the **SMB_COM_SESSION_SETUP_ANDX** message is used to attempt authentication. This message can include usernames, domains, and encrypted (or sometimes plaintext) passwords. Weak encryption schemes like **Lanman** or **NTLM** can be cracked using appropriate tools.

To automate SMB user enumeration, Nmap provides a useful script:  

```
nmap --script smb-enum-users.nse <target>
```  

This uses the `smb-enum-users.nse` script to extract a list of valid users from a host over SMB. You can also combine it with host discovery and OS detection:  

```
nmap -p445 --script smb-enum-users.nse -O <target>
``` 

Here, `-p445` specifies the port, and `-O` attempts OS detection.

---

#### Group Enumeration

For a penetration tester, **_group enumeration_** is helpful in determining the authorization roles that are being used in the target environment. The Nmap NSE script for enumerating SMB groups is **smb-enum-groups**. This script attempts to pull a list of groups from a remote Windows machine. You can also reveal the list of users who are members of those groups. The syntax of the command is as follows:

```
nmap --script smb-enum-groups.nse -p445 <target>
``` 

---

#### Network Share Enumeration

Identifying systems on a network that are sharing files, folders, or printers is essential for expanding the attack surface during internal reconnaissance. Network share enumeration can be efficiently done using the Nmap **`smb-enum-shares.nse`** script, which utilizes **Microsoft Remote Procedure Call (MSRPC)** to detect shared resources. The syntax to run this script is:  

```
nmap --script smb-enum-shares.nse -p 445 <host>
```

The `-p 445` option targets the SMB service running on port 445, which is common in Windows and Samba environments. For example, scanning the host at `192.168.88.251`, which runs Samba on Linux, can reveal accessible shares even when OS fingerprinting does not clearly indicate it's a Linux machine. 

For broader enumeration and OS fingerprinting, the command `nmap -sC <host>` can be used. The `-sC` option runs a default set of NSE scripts based on the open ports discovered, helping to gather more details about the target.

Another powerful tool for enumerating Samba shares is **enum4linux**, which retrieves information like usernames, shared directories, and server configuration from a target system. 

Running it against `192.168.88.251` provides extensive detail similar to what you'd get from the Nmap script, but with a Linux-friendly interface and output.

---

### Web Page Enumeration / Web Application Enumeration

After identifying that a target host is running a web server, the next step is to enumerate the web application and discover hidden directories and files—this is known as web page or web application enumeration. 

Nmap offers the **`http-enum`** NSE script for this purpose. The script probes a web server with a list of known file and directory paths from common web applications. Based on the server's responses, it can reveal whether those paths exist. This is particularly helpful in detecting administrative interfaces or default pages such as the **Tomcat manager** or **Apache test pages**. To run this script, use the command:  

```
nmap -sV --script=http-enum <target>
```

The `-sV` option enables service version detection, which helps identify the software running on the open web ports.

Another valuable tool for web enumeration is **Nikto**, an open-source web vulnerability scanner designed to scan web servers quickly and noisily. 

It can detect outdated server software, default files, and configuration issues, and it even supports basic authentication for web applications. Although not as advanced as commercial scanners, Nikto is useful for getting fast results. It can be launched with a simple command such as:  

```
nikto -h <host>
```

This will scan the target and display known vulnerabilities or configuration issues in the output.

---

### Service Enumeration

Service enumeration focuses on identifying what services are running on a system, their versions, and potentially the users or processes behind them. 

This is a core capability of Nmap, which uses different scan types to reveal as much information as possible—even through basic firewall filtering. On Windows systems, you can go a step further using the **`smb-enum-processes.nse`** script. 

This script requires valid SMB credentials to access process and service details from a remote machine. The command syntax is:  

```
nmap --script smb-enum-processes.nse --script-args smbusername=<username>,smbpass=<password> -p445 <host>
```

The `--script-args` option passes authentication details, while `-p445` targets the SMB port. This approach simulates a credentialed user querying services, offering visibility into the exact services and processes running on the target Windows host.

---

#### Exploring Enumeration via Packet Crafting (Scapy)

In more advanced scenarios, packet crafting provides fine-grained control over how network reconnaissance is performed. 

**Scapy**, a powerful Python-based framework, is widely used by penetration testers for custom packet generation and analysis. 

Scapy allows users to craft packets manually and analyze responses for detailed fingerprinting. To use Scapy, it must be run with root privileges, as modifying packets requires administrative access. 

You can launch the interactive shell with the following command:  

```
sudo scapy
```

Once inside Scapy, you can craft various protocol packets (IP, TCP, UDP, ICMP) and send them across the network to test firewall rules, protocol behavior, or service responses. It is a flexible tool for enumeration tasks where standard scanners might be blocked or limited.

Here are some common use cases of Scapy in enumeration:


1. **Ping Sweep with ICMP Echo Requests**

To discover live hosts on a subnet:

```python
ans, unans = sr(IP(dst="192.168.1.0/24")/ICMP(), timeout=2)
ans.summary()
```

This sends ICMP echo requests to the entire subnet. `ans.summary()` displays the hosts that responded.

---

2. **TCP SYN Port Scan**

To scan a specific host for open ports using a TCP SYN scan:

```python
sr1(IP(dst="192.168.1.100")/TCP(dport=80, flags="S"), timeout=1)
```

You can scan multiple ports:

```python
sr(IP(dst="192.168.1.100")/TCP(dport=[21,22,23,80,443], flags="S"), timeout=2)
```

---

3. **OS Fingerprinting via TTL and Window Size**

Send a SYN packet and analyze the TTL and window size:

```python
packet = sr1(IP(dst="192.168.1.100")/TCP(dport=80, flags="S"), timeout=2)
packet.ttl, packet.window
```

These values can help you fingerprint the operating system (e.g., TTL of 64 typically indicates Linux, 128 for Windows).

---

4. **Traceroute to a Host**

Scapy can mimic the behavior of `traceroute`:

```python
traceroute(["8.8.8.8"], maxttl=20)
```

---

5. **DNS Enumeration**

Scapy can craft custom DNS queries:

```python
dns_query = IP(dst="8.8.8.8")/UDP(dport=53)/DNS(rd=1, qd=DNSQR(qname="example.com"))
response = sr1(dns_query, timeout=2)
response.summary()
```

---

6. **ARP Scan (Layer 2 Discovery)**

To identify active hosts on a LAN:

```python
ans, unans = arping("192.168.1.0/24")
ans.summary()
```

This is useful for local network discovery when ICMP is blocked.

---

### Enumeration with Nmap

Nmap is a classic active reconnaissance tool for pentesters. Every pentester should be familiar with it because it has unparalleled flexibility. 

Nmap is a command line tool and Zenmap is a GUI version of it. While Zenmap makes knowledge of the wide range of Nmap options less crucial, Zenmap can not replace fluency at the Nmap command line.

**Nmap Common Scan Options**

|**Option**|**Description**|
|---|---|
|`-A`|Aggressive scan that enables OS detection, version detection, script scanning and traceroute|
|`-O`|Enables OS detection|
|`-p <port ranges>`|Allows for specific ports or port ranges to be scanned|
|`-sF`|Performs TCP FIN scan|
|`-sn`|Performs host discovery scan|
|`-sS`|Performs TCP SYN scan|
|`-sT`|Performs TCP Connect scan|
|`-sV`|Probes open ports to determine service/version info|
|`-T<0–5>`|Sets the timing of the scan. Higher numbers produce faster results. Slower scans elude detection better.|
|`-v`|Increases the verbosity of the output|
|`--open`|Only reports open (or possibly open) ports|

---


## Network Sniffing with Wireshark

Wireshark and tcpdump are indispensable tools for capturing traffic on a network. They enable detailed inspection of network traffic including passwords, hashes, files, and entire threaded packet exchanges. 

Wireshark uses a GUI for packet captures, while tcpdump is a command-line tool.


**1. Checking Network Configuration**

- Show current working directory:

    ```bash
    pwd
    ```

- View network interfaces and IP addresses:

    ```bash
    ifconfig
    ```

- Check routing table (to find the gateway):

    ```bash
    ip route
    ```

- Display configured DNS servers:

    ```bash
    cat /etc/resolv.conf
    ```


**2. Capturing Packets using tcpdump**

- Start packet capture on a specific interface and save to file:

    ```bash
    sudo tcpdump -i eth0 -s 0 -w packetdump.pcap
    ```

    - `-i eth0`: capture on eth0 interface
    
    - `-s 0`: capture full packet size
    
    - `-w packetdump.pcap`: save packets to a file


---

**3. Launching Wireshark**

- Start the Wireshark GUI:

    ```bash
    wireshark
    ```


**4. Analyzing Packets in Wireshark**

- To view only DNS traffic, use the filter:

    ```
    dns
    ```

- To search for a specific domain name:

- Press `Ctrl + F`

- Search for `skillsforall`

- To find HTTP POST requests:

 - Use `Ctrl + F` or filter with:

```
	POST
```


**5. Accessing the Saved Capture File**

- Check for the presence of the saved capture file:

    ```bash
    ls packetdump.pcap
    ```

---

More Notes of wireshark on `Wireshark.md`

---

### Typical Steps in an Automated Vulnerability Scan

|**Step**|**Description**|
|---|---|
|Step 1|In the discovery phase, the scanner uses a tool such as Nmap to perform host and port enumeration.|
|Step 2|The scanner records what software and version are running on an open port into a database for further analysis.|
|Step 3|The scanner tries to determine if the software that is listening on the target system is susceptible to any known vulnerabilities.|
|Step 4|The scanner produces a report on what it suspects could be vulnerable.|

---


## Types of Vulnerability Scans

Vulnerability scans are controlled by scan policies defined in automated tools. These tools often allow for full scans using all available options. 

However, in sensitive environments like production systems or unstable devices, only safe scan options should be selected to prevent crashes. 

The main types of vulnerability scans are **unauthenticated scans**, **authenticated scans**, **discovery scans**, **full scans**, **stealth scans**, and **compliance scans**.

#### Unauthenticated Scans

These scans are performed without any credentials. When only an IP address is provided, the scan mimics the perspective of an external attacker who does not have access to the system. 

It identifies only the services exposed to the network by checking for open ports. If a port is firewalled or not listening on the scanner’s network segment, it will appear closed. 

However, this doesn’t always mean it's not vulnerable — hidden ports can sometimes still be accessed using advanced methods like SSH port forwarding. Since unauthenticated scans provide limited internal visibility, they often produce more false positives compared to authenticated scans.

**Nmap Command Example**:

```
nmap -sS -T4 <target-IP>
```

- `-sS`: TCP SYN scan (stealthy and fast)

- `-T4`: Aggressive timing (faster scan)

#### Authenticated Scans

Authenticated scans involve logging into the target system using credentials with root or administrative privileges. This gives the scanner deeper insight into the system’s configuration and vulnerabilities. 

The scanner uses tools like `netstat` to list open ports and associated services. The results of these commands vary depending on the level of access. For instance, when run by a regular user, the `netstat` output hides information such as the PID and program name, whereas with root access, detailed process information is shown. 

This visibility makes authenticated scans more reliable and effective for identifying real threats.

**Tools**: Nessus, OpenVAS, Qualys

```
netstat -tulnp
```

- `-t`: TCP connections

- `-u`: UDP connections

- `-l`: Listening ports

- `-n`: Show numeric IPs/ports

- `-p`: Show process name (requires root)


**Authenticated Scanner Command Example (Nessus)**:

Nessus allows setting SSH or Windows credentials via its GUI or command-line.

 Example using SSH:

```
nessuscli scan --target <IP> --ssh-credentials <user>:<pass>
```

- Provides **more accurate and detailed results**, reduces false positives, and detects issues not visible to unauthenticated scans.

#### Discovery Scans

Discovery scans are primarily used to map out the target's attack surface. They start with a basic port scan, often using tools like Nmap, to determine which ports are open. 

Once open ports are identified, the scanner attempts to determine the exact services running on those ports. For example, it may detect an Apache Tomcat web server on ports 80 and 443 or an OpenSSH service on port 22. 

After identifying these services, the scanner conducts further probes tailored to each one, such as checking for outdated versions or weak configurations. 

Discovery scans provide foundational information that informs deeper vulnerability assessments.

**Nmap Command Example**:

```
nmap -sV -O -T4 <target-IP>
```

- `-sV`: Service/version detection

- `-O`: OS detection

- `-T4`: Speeds up scan


**Masscan Command (fastest port scanner)**:

```
masscan -p1-65535 <target-IP> --rate=10000
```

- Discovery scans are useful for **enumerating all accessible services** before running deeper scans.

#### Full Scans

A full scan is a comprehensive scan that activates every available scanning option defined in the tool’s scan policy. This includes a wide range of checks against services, configurations, user permissions, and more. Full scans are generally used in test environments rather than live systems, as they can be intrusive and might disrupt services or cause system instability if misconfigured.

```
nessuscli scan --policy "Full Scan" --target <IP>
```



---

#### Challenges to Consider When Running a Vulnerability Scan

1. **System and Network Performance Impact**  
    Vulnerability scans can consume significant bandwidth and system resources, potentially degrading network or application performance.

2. **False Positives and False Negatives**  
    Scans may report vulnerabilities that don't exist (false positives) or miss actual issues (false negatives), leading to mistrust in scan results or missed threats.

3. **Authentication Issues**  
    Lack of proper credentials in authenticated scans can limit visibility, while incorrect credentials may result in incomplete or failed scans.

4. **Access Restrictions**  
    Firewalls, IDS/IPS, and ACLs can block scan attempts, resulting in incomplete data.

5. **Disruption of Services**  
    Aggressive scanning (e.g., full scans) may disrupt critical services or trigger security defenses unintentionally.

6. **Scope Definition and Asset Inventory**  
    Poorly defined scope or outdated asset lists can lead to incomplete scanning or legal issues if unauthorized systems are probed.

7. **Scan Configuration Complexity**  
    Incorrect configuration of scan tools can lead to ineffective results or increased risk of disruption.

8. **Legal and Compliance Concerns**  
    Some scans may violate internal policies or regulatory requirements if not planned properly.

9. **Timing and Frequency**  
    Running scans at the wrong time (e.g., during peak hours) can affect operations; infrequent scans may miss new vulnerabilities.

10. **Data Overload and Prioritization**  
    Large volumes of results can overwhelm teams, making it hard to prioritize which vulnerabilities to address first.

---

